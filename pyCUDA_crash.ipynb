{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as drv\n",
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray as gpuarray\n",
    "\n",
    "drv.init()\n",
    "\n",
    "print(drv.get_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 设置随机数的数量：2^12\n",
    "num_elements = 2<<12\n",
    "\n",
    "# 在CPU上生成2^12个随机数，赋值给a_host\n",
    "a_host = np.random.rand(num_elements).astype(np.float32)\n",
    "b_host = np.random.rand(num_elements).astype(np.float32)\n",
    "c_host = a_host + b_host\n",
    "# 打印数组的一部分和数组的形状，确认生成成功\n",
    "print(\"Array shape:\", a_host.shape,b_host.shape)\n",
    "print(\"First 10 elements:\", a_host[:10],b_host[:10])\n",
    "\n",
    "# 将a_host和b_host复制到GPU上\n",
    "a_gpu = gpuarray.to_gpu(a_host)\n",
    "b_gpu = gpuarray.to_gpu(b_host)\n",
    "\n",
    "# 打印GPU数组的形状，确认复制成功\n",
    "print(\"GPU array shape:\", a_gpu.shape,b_gpu.shape)\n",
    "c_gpu = a_gpu + b_gpu\n",
    "c_gpu_host = c_gpu.get()\n",
    "print(\"First 10 elements:\", c_gpu_host[:10])\n",
    "\n",
    "#比较c_host和c_gpu每一个元素\n",
    "# Compare c_host with c_gpu\n",
    "are_equal = np.allclose(c_host, c_gpu_host)\n",
    "\n",
    "print(\"Are all elements equal?\", are_equal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy as np\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "# 定义CUDA kernel\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void add_vectors(float *dest, float *a, float *b)\n",
    "{\n",
    "    const int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    dest[i] = a[i] + b[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# 从module中获取kernel函数\n",
    "add_vectors = mod.get_function(\"add_vectors\")\n",
    "\n",
    "# 设置随机数的数量：2^12\n",
    "num_elements = 2 << 12\n",
    "\n",
    "# 在CPU上生成随机数，赋值给a_host和b_host\n",
    "a_host = np.random.rand(num_elements).astype(np.float32)\n",
    "b_host = np.random.rand(num_elements).astype(np.float32)\n",
    "c_host = a_host + b_host\n",
    "\n",
    "# 将a_host和b_host复制到GPU上\n",
    "# 分配GPU内存并复制数据\n",
    "a_gpu = drv.mem_alloc(a_host.nbytes)\n",
    "b_gpu = drv.mem_alloc(b_host.nbytes)\n",
    "c_gpu = drv.mem_alloc(a_host.nbytes)  # c的结果数组\n",
    "\n",
    "drv.memcpy_htod(a_gpu, a_host)\n",
    "drv.memcpy_htod(b_gpu, b_host)\n",
    "\n",
    "# 设置block和grid的大小\n",
    "block_size = (256, 1, 1)\n",
    "num_blocks = (num_elements + block_size[0] - 1) // block_size[0]\n",
    "\n",
    "# 调用kernel\n",
    "add_vectors(c_gpu, a_gpu, b_gpu, block=(block_size[0], 1, 1), grid=(num_blocks, 1))\n",
    "\n",
    "# 将结果复制回CPU\n",
    "c_host_gpu = np.empty_like(a_host)\n",
    "drv.memcpy_dtoh(c_host_gpu, c_gpu)\n",
    "\n",
    "# 比较结果\n",
    "are_equal = np.allclose(c_host, c_host_gpu)\n",
    "print(\"Are all elements equal?\", are_equal)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
